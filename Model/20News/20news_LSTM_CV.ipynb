{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 950 (CNMeM is enabled with initial size: 70.0% of memory, cuDNN 5005)\n",
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Embedding Dimesions: 25\n",
      "Found 1193514 word vectors.\n",
      "Processing text dataset\n",
      "Found 5000 texts.\n",
      "Found 101187 unique tokens.\n",
      "Shape of data tensor: (5000, 1000)\n",
      "Shape of label tensor: (5000, 5)\n",
      "Preparing embedding matrix.\n",
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "np.random.seed(1337)\n",
    "\n",
    "import theano\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Flatten, Activation\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Embedding, LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.optimizers import SGD, Adadelta\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sys\n",
    "\n",
    "BASE_DIR = '.'\n",
    "GLOVE_DIR = BASE_DIR + '/glove.twitter.27B/'\n",
    "\n",
    "TEXT_DATA_DIR = BASE_DIR + '/20_newsgroups/'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 25  #25, 50, 100, 200\n",
    "VALIDATION_SPLIT = 0.2\n",
    "DENSE_FEATURE = 1024\n",
    "DROP_OUT = 0.3\n",
    "\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "print('Embedding Dimesions: %s' % (str(EMBEDDING_DIM)))\n",
    "\n",
    "embeddings_index = {}\n",
    "fname = os.path.join(GLOVE_DIR, 'glove.twitter.27B.' + str(EMBEDDING_DIM) + 'd.txt')\n",
    "f = open(fname)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                if sys.version_info < (3,):\n",
    "                    f = open(fpath)\n",
    "                else:\n",
    "                    f = open(fpath, encoding='latin-1')\n",
    "                texts.append(f.read())\n",
    "                f.close()\n",
    "                labels.append(label_id)\n",
    "\n",
    "print('Found %s texts.' % len(texts))\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "# embedding_layer = Embedding(nb_words + 1,\n",
    "#                             EMBEDDING_DIM,\n",
    "#                             weights=[embedding_matrix],\n",
    "#                             input_length=MAX_SEQUENCE_LENGTH,\n",
    "#                             trainable=False)\n",
    "\n",
    "print('Training model.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(optimizer='sgd', dropout_rate= 0.2):\n",
    "\tstart = time.time()\n",
    "\tmodel = Sequential()\n",
    "\n",
    "\tmodel.add(Embedding(                          # Layer 0, Start\n",
    "    \t\tinput_dim=nb_words + 1,                   # Size to dictionary, has to be input + 1\n",
    "    \t\toutput_dim=EMBEDDING_DIM,                 # Dimensions to generate\n",
    "    \t\tweights=[embedding_matrix],               # Initialize word weights\n",
    "\t    \tinput_length=MAX_SEQUENCE_LENGTH))        # Define length to input sequences in the first layer\n",
    "\n",
    "\tmodel.add(LSTM(128, dropout_W=dropout_rate, dropout_U=dropout_rate))  # try using a GRU instead, for fun\n",
    "\tmodel.add(Dense(5))\n",
    "\tmodel.add(Activation('sigmoid'))\n",
    "\n",
    "\tmodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\treturn model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model,verbose=0)\n",
    "sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "optimizers = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "#learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "#activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, nb_epoch=epochs, optimizer=optimizers, \n",
    "\t\tdropout_rate=dropout_rate)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "lstm = GridSearchCV(estimator=model, param_grid=param_grid, cv=10) # Cross Validation for the best hyperparameters\n",
    "\n",
    "grid_result = lstm.fit(x_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "print (\"Fitting Time : \", time.time() - start)\n",
    "\n",
    "\n",
    "print(\"Done compiling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
