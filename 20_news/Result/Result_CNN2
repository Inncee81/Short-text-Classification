
CNN on 20News for 5 classes
************************************************************************************************
************************************************************************************************
{'acc': [0.19800000097602605, 0.2109999991953373, 0.20999999921768903, 0.21974999979138374, 0.22525000143796206, 0.23475000243633987, 0.2360000006854534, 0.24250000044703485, 0.24800000060349703, 0.25550000127404926, 0.24974999856203794, 0.25449999906122683, 0.26025000028312206, 0.26625000033527613, 0.2532500009983778, 0.26400000378489497, 0.25575000029057265, 0.25250000003725293, 0.26299999933689833, 0.26499999910593031, 0.26850000116974115, 0.26300000250339506, 0.26125000230967999, 0.26950000040233135, 0.26099999900907278], 'loss': [1.6104381382465363, 1.6080488964915276, 1.6071979358792305, 1.6063999265432358, 1.6042621657252312, 1.603026381134987, 1.6033478483557702, 1.6013985812664031, 1.5997702926397324, 1.5990361750125885, 1.5982592403888702, 1.5986132144927978, 1.5962367475032806, 1.5951484739780426, 1.5946302458643913, 1.5921904817223549, 1.5928056076169015, 1.591349121928215, 1.5891200244426726, 1.590068170428276, 1.5875675275921821, 1.5863704428076744, 1.5863887369632721, 1.5856153160333633, 1.5859652444720269], 'val_acc': [0.20000000223517417, 0.21800000220537186, 0.22000000029802322, 0.22800000086426736, 0.23500000163912774, 0.23900000080466272, 0.25500000268220901, 0.26099999919533728, 0.25499999970197679, 0.2560000032186508, 0.25499999672174456, 0.25200000032782555, 0.25100000351667406, 0.25099999904632569, 0.25699999853968619, 0.25100000202655792, 0.25600000172853471, 0.2600000023841858, 0.26100000590085981, 0.27100000083446502, 0.27000000178813932, 0.26900000274181368, 0.27200000733137131, 0.27600000128149987, 0.27800000160932542], 'val_loss': [1.6085421383380889, 1.6072449088096619, 1.6060135662555695, 1.6048307597637177, 1.6036922752857208, 1.6025858879089356, 1.6015147328376771, 1.600554919242859, 1.5994688153266907, 1.5984951496124267, 1.5975026488304138, 1.5965747475624084, 1.5956608474254608, 1.5947088122367858, 1.5938130974769593, 1.5928905308246613, 1.5920174956321715, 1.5911048769950866, 1.5901968717575072, 1.5893699347972869, 1.588578575849533, 1.587744528055191, 1.5869149923324586, 1.5861336171627045, 1.5854589104652406]}
************************************************************************************************
************************************************************************************************

Indexing word vectors.
Found 1193514 word vectors.
Processing text dataset
Found 5000 texts.
Found 101187 unique tokens.
Shape of data tensor: (5000, 1000)
Shape of label tensor: (5000, 5)
Preparing embedding matrix.
Training model.
Done compiling.
Train on 4000 samples, validate on 1000 samples
Epoch 1/25
4000/4000 [==============================] - 23s - loss: 1.6098 - acc: 0.2018 - val_loss: 1.6067 - val_acc: 0.2200
Epoch 2/25
4000/4000 [==============================] - 23s - loss: 1.6053 - acc: 0.2300 - val_loss: 1.6032 - val_acc: 0.2430
Epoch 3/25
4000/4000 [==============================] - 23s - loss: 1.6023 - acc: 0.2327 - val_loss: 1.6001 - val_acc: 0.2680
Epoch 4/25
4000/4000 [==============================] - 23s - loss: 1.5996 - acc: 0.2573 - val_loss: 1.5972 - val_acc: 0.2470
Epoch 5/25
4000/4000 [==============================] - 23s - loss: 1.5959 - acc: 0.2595 - val_loss: 1.5945 - val_acc: 0.2550
Epoch 6/25
4000/4000 [==============================] - 23s - loss: 1.5927 - acc: 0.2567 - val_loss: 1.5919 - val_acc: 0.2550
Epoch 7/25
4000/4000 [==============================] - 23s - loss: 1.5921 - acc: 0.2578 - val_loss: 1.5893 - val_acc: 0.2670
4000/4000 [==============================] - 23s - loss: 1.5882 - acc: 0.2580 - val_loss: 1.5871 - val_acc: 0.2740
Epoch 9/25
4000/4000 [==============================] - 23s - loss: 1.5843 - acc: 0.2687 - val_loss: 1.5847 - val_acc: 0.2790
Epoch 10/25
4000/4000 [==============================] - 23s - loss: 1.5826 - acc: 0.2658 - val_loss: 1.5829 - val_acc: 0.2760
Epoch 11/25
4000/4000 [==============================] - 23s - loss: 1.5811 - acc: 0.2705 - val_loss: 1.5812 - val_acc: 0.2740
Epoch 12/25
4000/4000 [==============================] - 23s - loss: 1.5815 - acc: 0.2678 - val_loss: 1.5799 - val_acc: 0.2730
Epoch 13/25
4000/4000 [==============================] - 23s - loss: 1.5773 - acc: 0.2675 - val_loss: 1.5787 - val_acc: 0.2710
Epoch 14/25
4000/4000 [==============================] - 23s - loss: 1.5752 - acc: 0.2775 - val_loss: 1.5776 - val_acc: 0.2730
4000/4000 [==============================] - 23s - loss: 1.5742 - acc: 0.2783 - val_loss: 1.5767 - val_acc: 0.2740
Epoch 16/25
4000/4000 [==============================] - 23s - loss: 1.5710 - acc: 0.2710 - val_loss: 1.5758 - val_acc: 0.2710
Epoch 17/25
4000/4000 [==============================] - 23s - loss: 1.5726 - acc: 0.2670 - val_loss: 1.5750 - val_acc: 0.2700
4000/4000 [==============================] - 23s - loss: 1.5702 - acc: 0.2705 - val_loss: 1.5743 - val_acc: 0.2690
Epoch 19/25
4000/4000 [==============================] - 23s - loss: 1.5671 - acc: 0.2810 - val_loss: 1.5737 - val_acc: 0.2720
Epoch 20/25
4000/4000 [==============================] - 23s - loss: 1.5693 - acc: 0.2813 - val_loss: 1.5731 - val_acc: 0.2710
Epoch 21/25
4000/4000 [==============================] - 23s - loss: 1.5660 - acc: 0.2760 - val_loss: 1.5724 - val_acc: 0.2720
Epoch 22/25
4000/4000 [==============================] - 23s - loss: 1.5643 - acc: 0.2883 - val_loss: 1.5718 - val_acc: 0.2680
Epoch 23/25
4000/4000 [==============================] - 23s - loss: 1.5644 - acc: 0.2783 - val_loss: 1.5711 - val_acc: 0.2730
Epoch 24/25
4000/4000 [==============================] - 23s - loss: 1.5641 - acc: 0.2762 - val_loss: 1.5703 - val_acc: 0.2710
Epoch 25/25
4000/4000 [==============================] - 23s - loss: 1.5646 - acc: 0.2780 - val_loss: 1.5699 - val_acc: 0.2750
Compilation Time :  600.950145006

************************************************************************************************
************************************************************************************************
BASE_DIR = '.'
GLOVE_DIR = BASE_DIR + '/glove.twitter.27B/'

TEXT_DATA_DIR = BASE_DIR + '/20_newsgroups/'

MAX_SEQUENCE_LENGTH = 1000
MAX_NB_WORDS = 20000
EMBEDDING_DIM = 100
VALIDATION_SPLIT = 0.2
CONVOLUTION_FEATURE = 256
DENSE_FEATURE = 1024
DROP_OUT = 0.3

# first, build index mapping words in the embeddings set
# to their embedding vector

print('Indexing word vectors.')

embeddings_index = {}
fname = os.path.join(GLOVE_DIR, 'glove.twitter.27B.' + str(EMBEDDING_DIM) + 'd.txt')
f = open(fname)
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('Found %s word vectors.' % len(embeddings_index))

# second, prepare text samples and their labels
print('Processing text dataset')

texts = []  # list of text samples
labels_index = {}  # dictionary mapping label name to numeric id
labels = []  # list of label ids
for name in sorted(os.listdir(TEXT_DATA_DIR)):
    path = os.path.join(TEXT_DATA_DIR, name)
    if os.path.isdir(path):
        label_id = len(labels_index)
        labels_index[name] = label_id
        for fname in sorted(os.listdir(path)):
            if fname.isdigit():
                fpath = os.path.join(path, fname)
                if sys.version_info < (3,):
                    f = open(fpath)
                else:
                    f = open(fpath, encoding='latin-1')
                texts.append(f.read())
                f.close()
                labels.append(label_id)

print('Found %s texts.' % len(texts))

# finally, vectorize the text samples into a 2D integer tensor
tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)

labels = to_categorical(np.asarray(labels))
print('Shape of data tensor:', data.shape)
print('Shape of label tensor:', labels.shape)

# split the data into a training set and a validation set
indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]
nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])

x_train = data[:-nb_validation_samples]
y_train = labels[:-nb_validation_samples]
x_val = data[-nb_validation_samples:]
y_val = labels[-nb_validation_samples:]

print('Preparing embedding matrix.')

# prepare embedding matrix
nb_words = min(MAX_NB_WORDS, len(word_index))
embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))
for word, i in word_index.items():
    if i > MAX_NB_WORDS:
        continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector

# load pre-trained word embeddings into an Embedding layer
# note that we set trainable = False so as to keep the embeddings fixed
# embedding_layer = Embedding(nb_words + 1,
#                             EMBEDDING_DIM,
#                             weights=[embedding_matrix],
#                             input_length=MAX_SEQUENCE_LENGTH,
#                             trainable=False)

print('Training model.')

model = Sequential()

model.add(Embedding(                          # Layer 0, Start
    input_dim=nb_words + 1,                   # Size to dictionary, has to be input + 1
    output_dim=EMBEDDING_DIM,                 # Dimensions to generate
    weights=[embedding_matrix],               # Initialize word weights
    input_length=MAX_SEQUENCE_LENGTH))        # Define length to input sequences in the first layer

model.add(Convolution1D(                      # Layer 1,   Features: 256, Kernel Size: 7
    nb_filter=CONVOLUTION_FEATURE,            # Number of kernels or number of filters to generate
    filter_length=7,                          # Size of kernels
    border_mode='valid',                      # Border = 'valid', cause kernel to reduce dimensions
    activation='relu'))                       # Activation function to use

model.add(MaxPooling1D(                       # Layer 1a,  Max Pooling: 3
    pool_length=3))                           # Size of kernels

model.add(Convolution1D(                      # Layer 2,   Features: 256, Kernel Size: 7
    nb_filter=CONVOLUTION_FEATURE,            # Number of kernels or number of filters to generate
    filter_length=7,                          # Size of kernels
    border_mode='valid',                      # Border = 'valid', cause kernel to reduce dimensions
    activation='relu'))                       # Activation function to use

model.add(MaxPooling1D(                       # Layer 2a,  Max Pooling: 3
    pool_length=3))                           # Size of kernels

model.add(Convolution1D(                      # Layer 3,   Features: 256, Kernel Size: 3
    nb_filter=CONVOLUTION_FEATURE,            # Number of kernels or number of filters to generate
    filter_length=3,                          # Size of kernels
    border_mode='valid',                      # Border = 'valid', cause kernel to reduce dimensions
    activation='relu'))                       # Activation function to use

model.add(Convolution1D(                      # Layer 4,   Features: 256, Kernel Size: 3
    nb_filter=CONVOLUTION_FEATURE,            # Number of kernels or number of filters to generate
    filter_length=3,                          # Size of kernels
    border_mode='valid',                      # Border = 'valid', cause kernel to reduce dimensions
    activation='relu'))                       # Activation function to use

model.add(Convolution1D(                      # Layer 5,   Features: 256, Kernel Size: 3
    nb_filter=CONVOLUTION_FEATURE,            # Number of kernels or number of filters to generate
    filter_length=3,                          # Size of kernels
    border_mode='valid',                      # Border = 'valid', cause kernel to reduce dimensions
    activation='relu'))                       # Activation function to use

model.add(Convolution1D(                      # Layer 6,   Features: 256, Kernel Size: 3
    nb_filter=CONVOLUTION_FEATURE,            # Number of kernels or number of filters to generate
    filter_length=5,                          # Size of kernels
    border_mode='valid',                      # Border = 'valid', cause kernel to reduce dimensions
    activation='relu'))                       # Activation function to use

model.add(MaxPooling1D(                       # Layer 6a,  Max Pooling: 3
    pool_length=3))                           # Size of kernels

model.add(Flatten())                          # Layer 7

model.add(Dense(                              # Layer 7a,  Output Size: 1024
    output_dim=DENSE_FEATURE,                 # Output dimension
    activation='relu'))                       # Activation function to use

model.add(Dropout(DROP_OUT))

model.add(Dense(                              # Layer 8,   Output Size: 1024
    output_dim=DENSE_FEATURE,                 # Output dimension
    activation='relu'))                       # Activation function to use

model.add(Dropout(DROP_OUT))

model.add(Dense(                              # Layer 9,  Output Size: Size Unique Labels, Final
    output_dim=len(labels_index),             # Output dimension
    activation='softmax')) 

#sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)
#adadelta = Adadelta(lr=1.0, rho=0.95, epsilon=1e-08)
sgd = SGD(lr=0.0003, momentum=0.9, nesterov=True)



model.compile(loss='categorical_crossentropy', optimizer=sgd,
              metrics=['accuracy'])

print("Done compiling.")

start = time.time()

history = model.fit(x_train, y_train, validation_data=(x_val, y_val),
          nb_epoch=25, batch_size=150)

print ("Compilation Time : ", time.time() - start)