{'acc': [0.049193649262386396, 0.054819352684512497, 0.061695212315978042, 0.064320540149475067, 0.064758094567174476, 0.068133516604825251, 0.07075884493943646, 0.07175896972872374, 0.071321415318010117, 0.07500937632943469, 0.075884485506438767, 0.074696837380344022, 0.079384923297485754, 0.08076009540820378, 0.080385048470991607, 0.079884985578493828, 0.082510313954088352, 0.081510188978047557, 0.084010501722962969, 0.083197899776121909, 0.082885360895957716, 0.080760094959250184, 0.082947868614893339, 0.086010751709507796, 0.084760595276040837], 'loss': [2.9967597222414621, 2.9937171413535131, 2.9904275374466782, 2.9864429472565486, 2.9822498501084835, 2.9761616821243759, 2.9691960059012277, 2.9617427015382063, 2.9577376295855857, 2.9539704345169477, 2.9485961870486057, 2.946425012565133, 2.943323919334655, 2.9422825629867035, 2.9406449671074189, 2.9376586772662607, 2.9370958871849777, 2.9348491230433038, 2.9351769383959003, 2.9331659977399402, 2.9323464939185628, 2.9281836116383979, 2.9265223327972456, 2.9243096561935009, 2.922616477548547], 'val_acc': [0.060765191297824456, 0.067516880063794207, 0.070017504064954494, 0.070017504037007836, 0.069767441469211822, 0.076019004753050914, 0.079269817022121852, 0.083270817860927424, 0.082270567827290017, 0.08302075605607534, 0.082770692940524707, 0.082520629685240765, 0.081020254764736635, 0.083520880434312772, 0.084771192770252884, 0.086521630526841004, 0.088522131270425089, 0.087771943768253022, 0.088772193044535846, 0.089272318269557199, 0.091522882816179832, 0.08877219354757579, 0.090022506218875878, 0.091772944254930627, 0.092023006420294678], 'val_loss': [2.9930958869487649, 2.9900710597041846, 2.9867050892771947, 2.9822957365475289, 2.976285687981024, 2.9684885945073303, 2.9595106812410577, 2.9516555014536601, 2.9464621771034762, 2.9428494425528227, 2.940061389997501, 2.9376522829366283, 2.9355884326282338, 2.934156568982238, 2.9325519112474652, 2.9311949417989949, 2.9296214653867936, 2.9280274570748399, 2.9266696131864349, 2.9249438534798875, 2.9232089084874215, 2.9212593345470386, 2.919253753524746, 2.916897784831912, 2.9140901801645174]}

###################################################################################################


Indexing word vectors.
Found 1193514 word vectors.
Processing text dataset
Found 19997 texts.
Found 214909 unique tokens.
Shape of data tensor: (19997, 1000)
Shape of label tensor: (19997, 20)
Preparing embedding matrix.
Training model.
Done compiling.
Train on 15998 samples, validate on 3999 samples
Epoch 1/25
15998/15998 [==============================] - 94s - loss: 2.9968 - acc: 0.0492 - val_loss: 2.9931 - val_acc: 0.0608
Epoch 2/25
15998/15998 [==============================] - 94s - loss: 2.9937 - acc: 0.0548 - val_loss: 2.9901 - val_acc: 0.0675
Epoch 3/25
15998/15998 [==============================] - 95s - loss: 2.9904 - acc: 0.0617 - val_loss: 2.9867 - val_acc: 0.0700
Epoch 4/25
15998/15998 [==============================] - 95s - loss: 2.9864 - acc: 0.0643 - val_loss: 2.9823 - val_acc: 0.0700
Epoch 5/25
15998/15998 [==============================] - 95s - loss: 2.9822 - acc: 0.0648 - val_loss: 2.9763 - val_acc: 0.0698
Epoch 6/25
15998/15998 [==============================] - 95s - loss: 2.9762 - acc: 0.0681 - val_loss: 2.9685 - val_acc: 0.0760
Epoch 7/25
15998/15998 [==============================] - 95s - loss: 2.9692 - acc: 0.0708 - val_loss: 2.9595 - val_acc: 0.0793
Epoch 8/25
15998/15998 [==============================] - 95s - loss: 2.9617 - acc: 0.0718 - val_loss: 2.9517 - val_acc: 0.0833
15998/15998 [==============================] - 95s - loss: 2.9577 - acc: 0.0713 - val_loss: 2.9465 - val_acc: 0.0823
Epoch 10/25
15998/15998 [==============================] - 95s - loss: 2.9540 - acc: 0.0750 - val_loss: 2.9428 - val_acc: 0.0830
Epoch 11/25
15998/15998 [==============================] - 95s - loss: 2.9486 - acc: 0.0759 - val_loss: 2.9401 - val_acc: 0.0828
Epoch 12/25
15998/15998 [==============================] - 95s - loss: 2.9464 - acc: 0.0747 - val_loss: 2.9377 - val_acc: 0.0825
Epoch 13/25
15998/15998 [==============================] - 95s - loss: 2.9433 - acc: 0.0794 - val_loss: 2.9356 - val_acc: 0.0810
Epoch 14/25
15998/15998 [==============================] - 95s - loss: 2.9423 - acc: 0.0808 - val_loss: 2.9342 - val_acc: 0.0835
Epoch 15/25
15998/15998 [==============================] - 95s - loss: 2.9406 - acc: 0.0804 - val_loss: 2.9326 - val_acc: 0.0848
Epoch 16/25
15998/15998 [==============================] - 95s - loss: 2.9377 - acc: 0.0799 - val_loss: 2.9312 - val_acc: 0.0865
Epoch 17/25
15998/15998 [==============================] - 96s - loss: 2.9371 - acc: 0.0825 - val_loss: 2.9296 - val_acc: 0.0885
Epoch 18/25
15998/15998 [==============================] - 95s - loss: 2.9348 - acc: 0.0815 - val_loss: 2.9280 - val_acc: 0.0878
Epoch 19/25
15998/15998 [==============================] - 95s - loss: 2.9352 - acc: 0.0840 - val_loss: 2.9267 - val_acc: 0.0888
Epoch 20/25
15998/15998 [==============================] - 95s - loss: 2.9332 - acc: 0.0832 - val_loss: 2.9249 - val_acc: 0.0893
Epoch 21/25
15998/15998 [==============================] - 95s - loss: 2.9323 - acc: 0.0829 - val_loss: 2.9232 - val_acc: 0.0915
Epoch 22/25
15998/15998 [==============================] - 95s - loss: 2.9282 - acc: 0.0808 - val_loss: 2.9213 - val_acc: 0.0888
Epoch 23/25
15998/15998 [==============================] - 96s - loss: 2.9265 - acc: 0.0829 - val_loss: 2.9193 - val_acc: 0.0900
Epoch 24/25
15998/15998 [==============================] - 95s - loss: 2.9243 - acc: 0.0860 - val_loss: 2.9169 - val_acc: 0.0918
Epoch 25/25
15998/15998 [==============================] - 95s - loss: 2.9226 - acc: 0.0848 - val_loss: 2.9141 - val_acc: 0.0920
Compilation Time :  2390.62611818
###################################################################################################

# coding: utf-8
from __future__ import print_function
import os
import numpy as np
import time
np.random.seed(1337)

import theano
import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical
from keras.layers import Dense, Flatten
from keras.layers import Convolution1D, MaxPooling1D, Embedding, LSTM
from keras.models import Model
from keras.layers import Input, Dropout
from keras.optimizers import SGD, Adadelta
from keras.models import Sequential
import sys

BASE_DIR = '.'
GLOVE_DIR = BASE_DIR + '/glove.twitter.27B/'

TEXT_DATA_DIR = BASE_DIR + '/20_newsgroupsf/'

MAX_SEQUENCE_LENGTH = 1000
MAX_NB_WORDS = 20000
EMBEDDING_DIM = 100
VALIDATION_SPLIT = 0.2
CONVOLUTION_FEATURE = 256
DENSE_FEATURE = 1024
DROP_OUT = 0.3

# first, build index mapping words in the embeddings set
# to their embedding vector

print('Indexing word vectors.')

embeddings_index = {}
fname = os.path.join(GLOVE_DIR, 'glove.twitter.27B.' + str(EMBEDDING_DIM) + 'd.txt')
f = open(fname)
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('Found %s word vectors.' % len(embeddings_index))

# second, prepare text samples and their labels
print('Processing text dataset')

texts = []  # list of text samples
labels_index = {}  # dictionary mapping label name to numeric id
labels = []  # list of label ids
for name in sorted(os.listdir(TEXT_DATA_DIR)):
    path = os.path.join(TEXT_DATA_DIR, name)
    if os.path.isdir(path):
        label_id = len(labels_index)
        labels_index[name] = label_id
        for fname in sorted(os.listdir(path)):
            if fname.isdigit():
                fpath = os.path.join(path, fname)
                if sys.version_info < (3,):
                    f = open(fpath)
                else:
                    f = open(fpath, encoding='latin-1')
                texts.append(f.read())
                f.close()
                labels.append(label_id)

print('Found %s texts.' % len(texts))

# finally, vectorize the text samples into a 2D integer tensor
tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)

labels = to_categorical(np.asarray(labels))
print('Shape of data tensor:', data.shape)
print('Shape of label tensor:', labels.shape)

# split the data into a training set and a validation set
indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]
nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])

x_train = data[:-nb_validation_samples]
y_train = labels[:-nb_validation_samples]
x_val = data[-nb_validation_samples:]
y_val = labels[-nb_validation_samples:]

print('Preparing embedding matrix.')

# prepare embedding matrix
nb_words = min(MAX_NB_WORDS, len(word_index))
embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))
for word, i in word_index.items():
    if i > MAX_NB_WORDS:
        continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector

# load pre-trained word embeddings into an Embedding layer
# note that we set trainable = False so as to keep the embeddings fixed
# embedding_layer = Embedding(nb_words + 1,
#                             EMBEDDING_DIM,
#                             weights=[embedding_matrix],
#                             input_length=MAX_SEQUENCE_LENGTH,
#                             trainable=False)

print('Training model.')

model = Sequential()

model.add(Embedding(                          # Layer 0, Start
    input_dim=nb_words + 1,                   # Size to dictionary, has to be input + 1
    output_dim=EMBEDDING_DIM,                 # Dimensions to generate
    weights=[embedding_matrix],               # Initialize word weights
    input_length=MAX_SEQUENCE_LENGTH))        # Define length to input sequences in the first layer

model.add(Convolution1D(                      # Layer 1,   Features: 256, Kernel Size: 7
    nb_filter=CONVOLUTION_FEATURE,            # Number of kernels or number of filters to generate
    filter_length=7,                          # Size of kernels
    border_mode='valid',                      # Border = 'valid', cause kernel to reduce dimensions
    activation='relu'))                       # Activation function to use

model.add(MaxPooling1D(                       # Layer 1a,  Max Pooling: 3
    pool_length=3))                           # Size of kernels

model.add(Convolution1D(                      # Layer 2,   Features: 256, Kernel Size: 7
    nb_filter=CONVOLUTION_FEATURE,            # Number of kernels or number of filters to generate
    filter_length=7,                          # Size of kernels
    border_mode='valid',                      # Border = 'valid', cause kernel to reduce dimensions
    activation='relu'))                       # Activation function to use

model.add(MaxPooling1D(                       # Layer 2a,  Max Pooling: 3
    pool_length=3))                           # Size of kernels

model.add(Convolution1D(                      # Layer 3,   Features: 256, Kernel Size: 3
    nb_filter=CONVOLUTION_FEATURE,            # Number of kernels or number of filters to generate
    filter_length=3,                          # Size of kernels
    border_mode='valid',                      # Border = 'valid', cause kernel to reduce dimensions
    activation='relu'))                       # Activation function to use

model.add(Convolution1D(                      # Layer 4,   Features: 256, Kernel Size: 3
    nb_filter=CONVOLUTION_FEATURE,            # Number of kernels or number of filters to generate
    filter_length=3,                          # Size of kernels
    border_mode='valid',                      # Border = 'valid', cause kernel to reduce dimensions
    activation='relu'))                       # Activation function to use

model.add(Convolution1D(                      # Layer 5,   Features: 256, Kernel Size: 3
    nb_filter=CONVOLUTION_FEATURE,            # Number of kernels or number of filters to generate
    filter_length=3,                          # Size of kernels
    border_mode='valid',                      # Border = 'valid', cause kernel to reduce dimensions
    activation='relu'))                       # Activation function to use

model.add(Convolution1D(                      # Layer 6,   Features: 256, Kernel Size: 3
    nb_filter=CONVOLUTION_FEATURE,            # Number of kernels or number of filters to generate
    filter_length=5,                          # Size of kernels
    border_mode='valid',                      # Border = 'valid', cause kernel to reduce dimensions
    activation='relu'))                       # Activation function to use

model.add(MaxPooling1D(                       # Layer 6a,  Max Pooling: 3
    pool_length=3))                           # Size of kernels

model.add(Flatten())                          # Layer 7

model.add(Dense(                              # Layer 7a,  Output Size: 1024
    output_dim=DENSE_FEATURE,                 # Output dimension
    activation='relu'))                       # Activation function to use

model.add(Dropout(DROP_OUT))

model.add(Dense(                              # Layer 8,   Output Size: 1024
    output_dim=DENSE_FEATURE,                 # Output dimension
    activation='relu'))                       # Activation function to use

model.add(Dropout(DROP_OUT))

model.add(Dense(                              # Layer 9,  Output Size: Size Unique Labels, Final
    output_dim=len(labels_index),             # Output dimension
    activation='softmax')) 

#sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)
#adadelta = Adadelta(lr=1.0, rho=0.95, epsilon=1e-08)
sgd = SGD(lr=0.0003, momentum=0.9, nesterov=True)



model.compile(loss='categorical_crossentropy', optimizer=sgd,
              metrics=['accuracy'])

print("Done compiling.")

start = time.time()

history = model.fit(x_train, y_train, validation_data=(x_val, y_val),
          nb_epoch=25, batch_size=150)

print ("Compilation Time : ", time.time() - start)